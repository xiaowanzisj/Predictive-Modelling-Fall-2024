{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mp\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from scipy.linalg import toeplitz\n",
    "from statsmodels.tsa.stattools import acf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('https://raw.githubusercontent.com/xiaowanzisj/Data-Science-1/refs/heads/main/dataset_lm.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 422 entries, 0 to 421\n",
      "Data columns (total 16 columns):\n",
      " #   Column               Non-Null Count  Dtype  \n",
      "---  ------               --------------  -----  \n",
      " 0   Dependent Var        422 non-null    float64\n",
      " 1   Explanatory Var #1   422 non-null    float64\n",
      " 2   Explanatory Var #2   422 non-null    float64\n",
      " 3   Explanatory Var #3   422 non-null    int64  \n",
      " 4   Explanatory Var #4   422 non-null    float64\n",
      " 5   Explanatory Var #5   422 non-null    float64\n",
      " 6   Explanatory Var #6   422 non-null    float64\n",
      " 7   Explanatory Var #7   422 non-null    float64\n",
      " 8   Explanatory Var #8   422 non-null    int64  \n",
      " 9   Explanatory Var #9   422 non-null    float64\n",
      " 10  Explanatory Var #10  422 non-null    float64\n",
      " 11  Explanatory Var #11  422 non-null    float64\n",
      " 12  Explanatory Var #12  422 non-null    float64\n",
      " 13  Explanatory Var #13  422 non-null    int64  \n",
      " 14  Explanatory Var #14  422 non-null    float64\n",
      " 15  Explanatory Var #15  422 non-null    float64\n",
      "dtypes: float64(13), int64(3)\n",
      "memory usage: 52.9 KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dependent Var</th>\n",
       "      <th>Explanatory Var #1</th>\n",
       "      <th>Explanatory Var #2</th>\n",
       "      <th>Explanatory Var #3</th>\n",
       "      <th>Explanatory Var #4</th>\n",
       "      <th>Explanatory Var #5</th>\n",
       "      <th>Explanatory Var #6</th>\n",
       "      <th>Explanatory Var #7</th>\n",
       "      <th>Explanatory Var #8</th>\n",
       "      <th>Explanatory Var #9</th>\n",
       "      <th>Explanatory Var #10</th>\n",
       "      <th>Explanatory Var #11</th>\n",
       "      <th>Explanatory Var #12</th>\n",
       "      <th>Explanatory Var #13</th>\n",
       "      <th>Explanatory Var #14</th>\n",
       "      <th>Explanatory Var #15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>56.293458</td>\n",
       "      <td>13.698667</td>\n",
       "      <td>50.639873</td>\n",
       "      <td>0</td>\n",
       "      <td>-18.568035</td>\n",
       "      <td>45.121911</td>\n",
       "      <td>11.412501</td>\n",
       "      <td>56.410757</td>\n",
       "      <td>2</td>\n",
       "      <td>-12.281132</td>\n",
       "      <td>38.996909</td>\n",
       "      <td>-3.010548</td>\n",
       "      <td>49.195073</td>\n",
       "      <td>0</td>\n",
       "      <td>-21.153143</td>\n",
       "      <td>46.919314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>58.473431</td>\n",
       "      <td>2.714725</td>\n",
       "      <td>65.845845</td>\n",
       "      <td>1</td>\n",
       "      <td>-25.105932</td>\n",
       "      <td>47.190213</td>\n",
       "      <td>10.080280</td>\n",
       "      <td>65.383107</td>\n",
       "      <td>3</td>\n",
       "      <td>-36.763585</td>\n",
       "      <td>51.654939</td>\n",
       "      <td>4.991111</td>\n",
       "      <td>45.591729</td>\n",
       "      <td>0</td>\n",
       "      <td>-6.474403</td>\n",
       "      <td>53.383508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>94.195330</td>\n",
       "      <td>11.618072</td>\n",
       "      <td>65.072497</td>\n",
       "      <td>0</td>\n",
       "      <td>-7.897464</td>\n",
       "      <td>52.163036</td>\n",
       "      <td>11.057301</td>\n",
       "      <td>82.812717</td>\n",
       "      <td>0</td>\n",
       "      <td>-15.733547</td>\n",
       "      <td>48.913837</td>\n",
       "      <td>-2.457696</td>\n",
       "      <td>56.608806</td>\n",
       "      <td>0</td>\n",
       "      <td>-27.903299</td>\n",
       "      <td>48.515026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>29.074583</td>\n",
       "      <td>0.818623</td>\n",
       "      <td>45.408996</td>\n",
       "      <td>1</td>\n",
       "      <td>-18.316132</td>\n",
       "      <td>54.356714</td>\n",
       "      <td>5.029029</td>\n",
       "      <td>48.812471</td>\n",
       "      <td>1</td>\n",
       "      <td>-12.825591</td>\n",
       "      <td>45.851732</td>\n",
       "      <td>14.974177</td>\n",
       "      <td>47.362594</td>\n",
       "      <td>1</td>\n",
       "      <td>-10.064411</td>\n",
       "      <td>55.266254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>86.035569</td>\n",
       "      <td>9.077544</td>\n",
       "      <td>73.548021</td>\n",
       "      <td>0</td>\n",
       "      <td>-19.204165</td>\n",
       "      <td>47.186807</td>\n",
       "      <td>12.128134</td>\n",
       "      <td>62.520911</td>\n",
       "      <td>2</td>\n",
       "      <td>-13.804860</td>\n",
       "      <td>47.765904</td>\n",
       "      <td>9.593982</td>\n",
       "      <td>53.700562</td>\n",
       "      <td>0</td>\n",
       "      <td>-17.546302</td>\n",
       "      <td>48.150543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417</th>\n",
       "      <td>61.300432</td>\n",
       "      <td>0.338441</td>\n",
       "      <td>70.430598</td>\n",
       "      <td>0</td>\n",
       "      <td>-21.525979</td>\n",
       "      <td>50.741115</td>\n",
       "      <td>-8.050843</td>\n",
       "      <td>39.075397</td>\n",
       "      <td>3</td>\n",
       "      <td>-29.197173</td>\n",
       "      <td>57.473862</td>\n",
       "      <td>15.505202</td>\n",
       "      <td>73.280605</td>\n",
       "      <td>1</td>\n",
       "      <td>-3.602482</td>\n",
       "      <td>56.317474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>418</th>\n",
       "      <td>26.309237</td>\n",
       "      <td>-1.729712</td>\n",
       "      <td>47.087996</td>\n",
       "      <td>1</td>\n",
       "      <td>-19.034807</td>\n",
       "      <td>55.242929</td>\n",
       "      <td>28.001769</td>\n",
       "      <td>76.429649</td>\n",
       "      <td>0</td>\n",
       "      <td>-21.941401</td>\n",
       "      <td>54.641620</td>\n",
       "      <td>14.295688</td>\n",
       "      <td>49.816798</td>\n",
       "      <td>1</td>\n",
       "      <td>-13.910494</td>\n",
       "      <td>52.827988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>419</th>\n",
       "      <td>58.350627</td>\n",
       "      <td>18.322301</td>\n",
       "      <td>53.267835</td>\n",
       "      <td>0</td>\n",
       "      <td>-26.186201</td>\n",
       "      <td>36.702958</td>\n",
       "      <td>14.530116</td>\n",
       "      <td>51.275150</td>\n",
       "      <td>2</td>\n",
       "      <td>-20.699670</td>\n",
       "      <td>46.443687</td>\n",
       "      <td>5.708762</td>\n",
       "      <td>52.751016</td>\n",
       "      <td>1</td>\n",
       "      <td>-6.202345</td>\n",
       "      <td>52.467289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>420</th>\n",
       "      <td>31.954003</td>\n",
       "      <td>0.436357</td>\n",
       "      <td>61.844132</td>\n",
       "      <td>0</td>\n",
       "      <td>-25.881961</td>\n",
       "      <td>57.106851</td>\n",
       "      <td>21.786066</td>\n",
       "      <td>68.928442</td>\n",
       "      <td>0</td>\n",
       "      <td>-23.166461</td>\n",
       "      <td>59.194840</td>\n",
       "      <td>8.365762</td>\n",
       "      <td>64.291056</td>\n",
       "      <td>1</td>\n",
       "      <td>-21.330017</td>\n",
       "      <td>50.237673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>421</th>\n",
       "      <td>50.590958</td>\n",
       "      <td>-3.107425</td>\n",
       "      <td>64.590632</td>\n",
       "      <td>1</td>\n",
       "      <td>-20.303215</td>\n",
       "      <td>56.374124</td>\n",
       "      <td>4.604237</td>\n",
       "      <td>49.501005</td>\n",
       "      <td>0</td>\n",
       "      <td>-18.792745</td>\n",
       "      <td>45.594612</td>\n",
       "      <td>16.414449</td>\n",
       "      <td>66.957547</td>\n",
       "      <td>0</td>\n",
       "      <td>-15.771155</td>\n",
       "      <td>43.603802</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>422 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Dependent Var  Explanatory Var #1  Explanatory Var #2  \\\n",
       "0        56.293458           13.698667           50.639873   \n",
       "1        58.473431            2.714725           65.845845   \n",
       "2        94.195330           11.618072           65.072497   \n",
       "3        29.074583            0.818623           45.408996   \n",
       "4        86.035569            9.077544           73.548021   \n",
       "..             ...                 ...                 ...   \n",
       "417      61.300432            0.338441           70.430598   \n",
       "418      26.309237           -1.729712           47.087996   \n",
       "419      58.350627           18.322301           53.267835   \n",
       "420      31.954003            0.436357           61.844132   \n",
       "421      50.590958           -3.107425           64.590632   \n",
       "\n",
       "     Explanatory Var #3  Explanatory Var #4  Explanatory Var #5  \\\n",
       "0                     0          -18.568035           45.121911   \n",
       "1                     1          -25.105932           47.190213   \n",
       "2                     0           -7.897464           52.163036   \n",
       "3                     1          -18.316132           54.356714   \n",
       "4                     0          -19.204165           47.186807   \n",
       "..                  ...                 ...                 ...   \n",
       "417                   0          -21.525979           50.741115   \n",
       "418                   1          -19.034807           55.242929   \n",
       "419                   0          -26.186201           36.702958   \n",
       "420                   0          -25.881961           57.106851   \n",
       "421                   1          -20.303215           56.374124   \n",
       "\n",
       "     Explanatory Var #6  Explanatory Var #7  Explanatory Var #8  \\\n",
       "0             11.412501           56.410757                   2   \n",
       "1             10.080280           65.383107                   3   \n",
       "2             11.057301           82.812717                   0   \n",
       "3              5.029029           48.812471                   1   \n",
       "4             12.128134           62.520911                   2   \n",
       "..                  ...                 ...                 ...   \n",
       "417           -8.050843           39.075397                   3   \n",
       "418           28.001769           76.429649                   0   \n",
       "419           14.530116           51.275150                   2   \n",
       "420           21.786066           68.928442                   0   \n",
       "421            4.604237           49.501005                   0   \n",
       "\n",
       "     Explanatory Var #9  Explanatory Var #10  Explanatory Var #11  \\\n",
       "0            -12.281132            38.996909            -3.010548   \n",
       "1            -36.763585            51.654939             4.991111   \n",
       "2            -15.733547            48.913837            -2.457696   \n",
       "3            -12.825591            45.851732            14.974177   \n",
       "4            -13.804860            47.765904             9.593982   \n",
       "..                  ...                  ...                  ...   \n",
       "417          -29.197173            57.473862            15.505202   \n",
       "418          -21.941401            54.641620            14.295688   \n",
       "419          -20.699670            46.443687             5.708762   \n",
       "420          -23.166461            59.194840             8.365762   \n",
       "421          -18.792745            45.594612            16.414449   \n",
       "\n",
       "     Explanatory Var #12  Explanatory Var #13  Explanatory Var #14  \\\n",
       "0              49.195073                    0           -21.153143   \n",
       "1              45.591729                    0            -6.474403   \n",
       "2              56.608806                    0           -27.903299   \n",
       "3              47.362594                    1           -10.064411   \n",
       "4              53.700562                    0           -17.546302   \n",
       "..                   ...                  ...                  ...   \n",
       "417            73.280605                    1            -3.602482   \n",
       "418            49.816798                    1           -13.910494   \n",
       "419            52.751016                    1            -6.202345   \n",
       "420            64.291056                    1           -21.330017   \n",
       "421            66.957547                    0           -15.771155   \n",
       "\n",
       "     Explanatory Var #15  \n",
       "0              46.919314  \n",
       "1              53.383508  \n",
       "2              48.515026  \n",
       "3              55.266254  \n",
       "4              48.150543  \n",
       "..                   ...  \n",
       "417            56.317474  \n",
       "418            52.827988  \n",
       "419            52.467289  \n",
       "420            50.237673  \n",
       "421            43.603802  \n",
       "\n",
       "[422 rows x 16 columns]"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dependent Var          0\n",
       "Explanatory Var #1     0\n",
       "Explanatory Var #2     0\n",
       "Explanatory Var #3     0\n",
       "Explanatory Var #4     0\n",
       "Explanatory Var #5     0\n",
       "Explanatory Var #6     0\n",
       "Explanatory Var #7     0\n",
       "Explanatory Var #8     0\n",
       "Explanatory Var #9     0\n",
       "Explanatory Var #10    0\n",
       "Explanatory Var #11    0\n",
       "Explanatory Var #12    0\n",
       "Explanatory Var #13    0\n",
       "Explanatory Var #14    0\n",
       "Explanatory Var #15    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part A (25%)\n",
    "1.\tDownload the “dataset_lm.csv” file from Canvas and upload it to Jupyter Notebook. \n",
    "2.\tRun the OLS model by using the dependent and explanatory variables in the dataset. \n",
    "3.\tShow your summary table in Python and interpret your results in the summary report.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['Dependent Var']\n",
    "x = df.drop('Dependent Var', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_updated = sm.add_constant(x)\n",
    "model_updated = sm.OLS(y,x_updated).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:          Dependent Var   R-squared:                       1.000\n",
      "Model:                            OLS   Adj. R-squared:                  1.000\n",
      "Method:                 Least Squares   F-statistic:                 1.682e+30\n",
      "Date:                Fri, 18 Oct 2024   Prob (F-statistic):               0.00\n",
      "Time:                        23:30:36   Log-Likelihood:                 12021.\n",
      "No. Observations:                 422   AIC:                        -2.401e+04\n",
      "Df Residuals:                     406   BIC:                        -2.395e+04\n",
      "Df Model:                          15                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "=======================================================================================\n",
      "                          coef    std err          t      P>|t|      [0.025      0.975]\n",
      "---------------------------------------------------------------------------------------\n",
      "const                  32.0000   9.19e-14   3.48e+14      0.000      32.000      32.000\n",
      "Explanatory Var #1      1.3000   7.42e-16   1.75e+15      0.000       1.300       1.300\n",
      "Explanatory Var #2      1.7000   5.39e-16   3.15e+15      0.000       1.700       1.700\n",
      "Explanatory Var #3      6.2000   1.03e-14   5.99e+14      0.000       6.200       6.200\n",
      "Explanatory Var #4      2.1000   6.37e-16    3.3e+15      0.000       2.100       2.100\n",
      "Explanatory Var #5     -0.9000   7.47e-16  -1.21e+15      0.000      -0.900      -0.900\n",
      "Explanatory Var #6   2.276e-15   4.51e-16      5.051      0.000    1.39e-15    3.16e-15\n",
      "Explanatory Var #7   3.018e-16   3.48e-16      0.867      0.386   -3.82e-16    9.86e-16\n",
      "Explanatory Var #8  -6.217e-15   4.73e-15     -1.314      0.189   -1.55e-14    3.08e-15\n",
      "Explanatory Var #9  -9.923e-16   6.05e-16     -1.641      0.102   -2.18e-15    1.97e-16\n",
      "Explanatory Var #10 -3.691e-15   7.54e-16     -4.897      0.000   -5.17e-15   -2.21e-15\n",
      "Explanatory Var #11  1.277e-15   7.29e-16      1.751      0.081   -1.56e-16    2.71e-15\n",
      "Explanatory Var #12 -9.437e-16   5.45e-16     -1.730      0.084   -2.02e-15    1.28e-16\n",
      "Explanatory Var #13 -3.553e-15   1.04e-14     -0.340      0.734   -2.41e-14     1.7e-14\n",
      "Explanatory Var #14 -1.596e-15   6.41e-16     -2.488      0.013   -2.86e-15   -3.35e-16\n",
      "Explanatory Var #15 -3.358e-15   7.64e-16     -4.397      0.000   -4.86e-15   -1.86e-15\n",
      "==============================================================================\n",
      "Omnibus:                        2.439   Durbin-Watson:                   0.781\n",
      "Prob(Omnibus):                  0.295   Jarque-Bera (JB):                2.053\n",
      "Skew:                           0.044   Prob(JB):                        0.358\n",
      "Kurtosis:                       2.670   Cond. No.                     2.55e+03\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 2.55e+03. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n"
     ]
    }
   ],
   "source": [
    "print(model_updated.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. R-squared (1.000) and Adjusted R-squared (1.000):\n",
    "The R-squared value in a regression model measures how well the independent variables explain the variability in the dependent variable. A value of 1.000 (or 100%) means that explanatory variables perfectly predict the dependent variable, which means that the model captures all possible variations in the dependent variable and leaves no room for error or randomness. The model maybe overfitting the data. \n",
    "The Adjusted R-squared is 1.000, which indicates that the model perfectly explains the variability of the dependent variable, even after adjusting for the number of predictors.\n",
    "\n",
    "2. F-statistic (1.682e+30) and Prob (F-statistic) (0.000):\n",
    "The F-statistic tests whether the independent variables, as a group, significantly predict the dependent variable. A very large F-statistic with a Prob (F-statistic) of 0.000 indicates that the model is statistically significant, meaning the explanatory variables jointly have a strong influence on the dependent variable.\n",
    "\n",
    "3. Coefficients (coef):\n",
    "•\tSignificant coefficients include:\n",
    "o\tExplanatory Var #3: the most significant positive coefficient of 6.200, which means a 1-unit increase in this explanatory variable leads to an increase of 6.200 in the dependent variable, assuming all other variables are held constant.\n",
    "o\tExplanatory Var #5: the most significant negative coefficient of -0.900, indicating a negative relationship, where an increase in this variable decreases the dependent variable by 0.900.\n",
    "These variables should be considered carefully when making decisions based on the model, as they are the most influential in either direction.\n",
    "4. P-values (P>|t|):\n",
    "•\tVariables with p-values less than 0.05: If the p-value is less than 0.05, the variable is considered statistically significant, which means that the variable has a meaningful impact on the dependent variable. Explanatory variable #1,2,3,4,5,6,10,14,15.\n",
    "•\tVariables with p-values greater than 0.05 (like Explanatory Var #7, 8, 9, 11,12,13) are not statistically significant and do not have a strong impact on the dependent variable.\n",
    "\n",
    "5. Standard Errors:\n",
    "•\tThe standard error (std err) indicates how much variation exists in the coefficient estimate. Smaller standard errors suggest higher precision and confidence in the estimate of that variable.\n",
    "•\tSmaller values indicate more precise estimates of the corresponding coefficients, whereas larger values suggest more variability in the estimate. Variables like #3, #8, and #13 have larger standard errors, implying more uncertainty in their coefficient estimates, while variables like #1, #2, and #6 have very small standard errors, indicating higher precision.\n",
    "6. T-statistics:\n",
    "•  T-statistics help determine if a variable is important in predicting the outcome. The bigger the t-value (positive or negative), the more significant that variable is. If the t-value is small or close to 0, the variable probably doesn't have much impact on the dependent variable. Based on the OLS regression results, Explanatory Var #1 is highly significant with a t-value of 1.75e+15, indicating a strong influence on the dependent variable, while Explanatory Var #7 shows a t-value of 0.867, suggesting it has little to no significant impact on the model's prediction.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part B (25%)\n",
    "1.\tUse error values from the OLS model to calculate their standard deviation and autocorrelation values for the first three lags. \n",
    "2.\tThen, run the GLS model accordingly. \n",
    "3.\tShow your summary table in Python and interpret your results in the summary report.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(6.591964783129596e-14)"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q1\n",
    "residuals = model_updated.resid\n",
    "std_deviation = residuals.std()\n",
    "std_deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.        ,  0.04566151, -0.02558894, -0.0018177 ])"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acf_residuals = acf(residuals, nlags = 3)\n",
    "acf_residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            GLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:          Dependent Var   R-squared:                       1.000\n",
      "Model:                            GLS   Adj. R-squared:                  1.000\n",
      "Method:                 Least Squares   F-statistic:                 5.089e+29\n",
      "Date:                Fri, 18 Oct 2024   Prob (F-statistic):               0.00\n",
      "Time:                        23:30:36   Log-Likelihood:                 11769.\n",
      "No. Observations:                 422   AIC:                        -2.351e+04\n",
      "Df Residuals:                     406   BIC:                        -2.344e+04\n",
      "Df Model:                          15                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "=======================================================================================\n",
      "                          coef    std err          t      P>|t|      [0.025      0.975]\n",
      "---------------------------------------------------------------------------------------\n",
      "const                  32.0000   1.67e-13   1.91e+14      0.000      32.000      32.000\n",
      "Explanatory Var #1      1.3000   1.35e-15   9.64e+14      0.000       1.300       1.300\n",
      "Explanatory Var #2      1.7000   9.81e-16   1.73e+15      0.000       1.700       1.700\n",
      "Explanatory Var #3      6.2000   1.88e-14    3.3e+14      0.000       6.200       6.200\n",
      "Explanatory Var #4      2.1000   1.16e-15   1.81e+15      0.000       2.100       2.100\n",
      "Explanatory Var #5     -0.9000   1.36e-15  -6.61e+14      0.000      -0.900      -0.900\n",
      "Explanatory Var #6   3.608e-16   8.19e-16      0.441      0.660   -1.25e-15    1.97e-15\n",
      "Explanatory Var #7  -1.023e-15   6.33e-16     -1.617      0.107   -2.27e-15    2.21e-16\n",
      "Explanatory Var #8   1.443e-15    8.6e-15      0.168      0.867   -1.55e-14    1.84e-14\n",
      "Explanatory Var #9  -2.776e-16    1.1e-15     -0.253      0.801   -2.44e-15    1.88e-15\n",
      "Explanatory Var #10   1.11e-16   1.37e-15      0.081      0.935   -2.57e-15     2.8e-15\n",
      "Explanatory Var #11 -6.106e-16   1.32e-15     -0.461      0.645   -3.21e-15    1.99e-15\n",
      "Explanatory Var #12 -3.442e-15   9.89e-16     -3.478      0.001   -5.39e-15    -1.5e-15\n",
      "Explanatory Var #13 -7.105e-15   1.89e-14     -0.376      0.707   -4.43e-14    3.01e-14\n",
      "Explanatory Var #14 -5.551e-17   1.17e-15     -0.048      0.962   -2.35e-15    2.24e-15\n",
      "Explanatory Var #15 -1.082e-15   1.38e-15     -0.783      0.434    -3.8e-15    1.63e-15\n",
      "==============================================================================\n",
      "Omnibus:                       15.230   Durbin-Watson:                   0.116\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):                7.348\n",
      "Skew:                           0.046   Prob(JB):                       0.0254\n",
      "Kurtosis:                       2.360   Cond. No.                     2.44e+03\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 2.44e+03. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n"
     ]
    }
   ],
   "source": [
    "# Q2\n",
    "rho = acf_residuals[1]\n",
    "n = len(y)\n",
    "cov_matrix = std_deviation**2 * toeplitz(np.append([1, rho], np.zeros(n-2)))\n",
    "print(sm.GLS(y, x_updated, sigma=cov_matrix).fit().summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. R-squared (1.000) & Adjusted R-squared (1.000):\n",
    "Both R-squared and Adjusted R-squared are 1.000, which means that the model explains 100% of the variation in the dependent variable. This is highly unusual and often indicates overfitting, where the model is too perfectly fitted to the data and may not generalize well to new data.\n",
    "\n",
    "2. F-statistic (5.089e+29) & Prob (F-statistic) (0.00):\n",
    "F-statistic (5.089e+29): This is an extremely high value, meaning the variables in your model are likely doing a good job predicting the outcome. Probability (F-statistic) (0.00): The probability of this F-statistic happening by chance is essentially zero. This means the model is highly significant, and the variables together are important in explaining the outcome.\n",
    "\n",
    "3. Coefficients (coef):\n",
    "The GLS regression model shows that several explanatory variables, such as Var #1, #2, #3, and #4, have significant positive coefficients, indicating a strong positive correlation with the dependent variable. In contrast, Var #5 and Var #12 have significant negative coefficients, suggesting a negative correlation with the dependent variable. Other explanatory variables, such as Var #6, #7, #8, #9, and #10, are not statistically significant, implying they have little influence on the model. Overall, the R-squared value of 1.000 indicates a very high fit, but it may also suggest the risk of overfitting\n",
    "\n",
    "4. P-values (P>|t|)：\n",
    "The p-values help in identifying which variables have a statistically significant effect on the outcome of the model. Explanatory Var #1, #2, #3, #4, #5, and #12 all have p-values of 0.000 or close to it, indicating that these variables are statistically significant at the 0.05 level. Explanatory Var #6, #7, #8, #9, #10, #11, #13, #14, and #15 have higher p-values (greater than 0.05), indicating they are not statistically significant in this model\n",
    "\n",
    "5. Standard Errors:\n",
    "The standard errors in GLS regression output are very small, indicating high precision in estimating the coefficients. Each variable's standard error shows little variability, confirming the accuracy of the model's estimates.\n",
    "\n",
    "6. T-statistics:\n",
    "The large t-statistics for most coefficients, combined with their very low p-values, suggest that these variables are statistically significant and play an important role in explaining the dependent variable. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part C (25%)\n",
    "1.\tSplit the dataset into two as the training and test sets (test size = 0.5). \n",
    "2.\tRun the Lasso model with alpha=1 and estimate the coefficients using the training set. \n",
    "3.\tThen, calculate the mean absolute percentage error using the test set. \n",
    "4.\tFind an approximate value for alpha that minimizes the mean absolute percentage error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.metrics import mean_absolute_percentage_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into two as the training and test sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.26972628,  1.68394638,  2.02626245,  2.08756512, -0.91746375,\n",
       "       -0.        ,  0.        , -0.        ,  0.        , -0.        ,\n",
       "        0.01314162,  0.        , -0.        ,  0.        , -0.03617731])"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lasso = Lasso(alpha=1)\n",
    "lasso.fit(x_train, y_train)\n",
    "lasso.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lasso has set some coefficients to zero, indicating that these variables may not contribute significantly to the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.04432190198291579)"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = lasso.predict(x_test)\n",
    "mape = mean_absolute_percentage_error(y_test, y_pred)\n",
    "mape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The Mean Absolute Percentage Error (MAPE), which indicates the average percentage error between the predicted and actual values in the test set for the Lasso model on the test set, is approximately 4.43%. \n",
    "2. A 4.43% MAPE number seems reasonable, only depending on the context and the specific field or industry where the data comes from. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best alpha: 0.001\n",
      "Lowest MAPE: 4.4128318074269805e-05\n"
     ]
    }
   ],
   "source": [
    "# Try alpha values between 0.001 and 100\n",
    "alphas = np.logspace(-3, 2, 100)  \n",
    "best_alpha = 1\n",
    "best_mape = float('inf')\n",
    "\n",
    "for alpha in alphas:\n",
    "    lasso = Lasso(alpha=alpha)\n",
    "    lasso.fit(x_train, y_train)\n",
    "    y_pred = lasso.predict(x_test)\n",
    "    mape = mean_absolute_percentage_error(y_test, y_pred)\n",
    "    \n",
    "    if mape < best_mape:\n",
    "        best_mape = mape\n",
    "        best_alpha = alpha\n",
    "\n",
    "print(f\"Best alpha: {best_alpha}\")\n",
    "print(f\"Lowest MAPE: {best_mape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The MAPE value decreases significantly to 4.4128318074115733e-05, when alpha = 0.001. \n",
    "2. With alpha set to 0.001, the regularization term is very weak. This means the model is almost unconstrained in terms of how large the coefficients can become. The model behaves much like an OLS model with only slight regularization.\n",
    "3. An alpha as low as 0.001 might be appropriate if you have a large, noise-free dataset and believe that all features are likely to contribute meaningfully to the outcome.However, if the goal is to simplify the model or improve generalizability, a slightly higher alpha would usually be preferable to avoid overfitting and to keep the model more interpretable and robust."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.29954993e+00,  1.69972328e+00,  6.07547327e+00,  2.09998671e+00,\n",
       "       -9.01138066e-01, -2.76774698e-04,  1.39022392e-04, -1.29553546e-03,\n",
       "        1.28574039e-04, -1.91998438e-04,  9.64691394e-04,  1.51252792e-04,\n",
       "       -7.24514808e-03,  4.88963422e-05, -1.75560566e-03])"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply Ridge Regression to our data\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn import linear_model as lm\n",
    "model_ridge = lm.Ridge(alpha=1).fit(x_train,y_train)\n",
    "model_ridge.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the test set using the fitted Ridge model\n",
    "y_pred_ridge = model_ridge.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.0013183099780278381)"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the MAPE for the Ridge model\n",
    "mape_ridge = mean_absolute_percentage_error(y_test, y_pred_ridge)\n",
    "mape_ridge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The MAPE decreased significantly to 0.13%, much lower than the Lasso model when the MAPE is 4.43% where alpha was set at 1. \n",
    "2. Based on the comparison, we can conclude that Lasso is sensitive to the scale of alpha, and small changes in alpha can lead to significant differences in results. While Ridge provides more accurate predictions on this dataset when alpha is the same value with Lasso. This could be due to Ridge Regression’s ability to handle multicollinearity without setting coefficients to zero, effectively utilizing all features for a smoother fit. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part D (25%)\n",
    "1.\tUse the demand data given in the table and develop an appropriate forecasting model (i.e., the tailored regularization discussed in the class—see your slides for more info) that exploits the available information given in the table as much as possible.\n",
    "2.\tInterpret your results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    'Month': list(range(1, 26)),\n",
    "    'Demand': [100, 112, 107, 103, 91, 85, 84, 85, 79, 81, 134, 86, 99, 89, 111, 114, 118, 163, 193, 143, 144, 202, 158, 160, 144],\n",
    "    'Advance_demand': [71, 30, 75, 64, 41, 51, 42, 51, 57, 49, 134, 52, 99, 56, 81, 79, 73, 163, 193, 99, 91, 202, 105, 101, 96]\n",
    "}\n",
    "\n",
    "df_d = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Month</th>\n",
       "      <th>Demand</th>\n",
       "      <th>Advance_demand</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>112</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>107</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>103</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>91</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>85</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>84</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>85</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>79</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>81</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>134</td>\n",
       "      <td>134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>86</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>89</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>111</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>114</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>118</td>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>163</td>\n",
       "      <td>163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>193</td>\n",
       "      <td>193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>143</td>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>144</td>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>202</td>\n",
       "      <td>202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>158</td>\n",
       "      <td>105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>160</td>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>144</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Month  Demand  Advance_demand\n",
       "0       1     100              71\n",
       "1       2     112              30\n",
       "2       3     107              75\n",
       "3       4     103              64\n",
       "4       5      91              41\n",
       "5       6      85              51\n",
       "6       7      84              42\n",
       "7       8      85              51\n",
       "8       9      79              57\n",
       "9      10      81              49\n",
       "10     11     134             134\n",
       "11     12      86              52\n",
       "12     13      99              99\n",
       "13     14      89              56\n",
       "14     15     111              81\n",
       "15     16     114              79\n",
       "16     17     118              73\n",
       "17     18     163             163\n",
       "18     19     193             193\n",
       "19     20     143              99\n",
       "20     21     144              91\n",
       "21     22     202             202\n",
       "22     23     158             105\n",
       "23     24     160             101\n",
       "24     25     144              96"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error: 16.12293300907556\n",
      "Model Coefficients: [0.69681898]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "X = df_d[['Advance_demand']]  # 自变量 (特征)\n",
    "y = df_d['Demand']  # 因变量 (目标)\n",
    "\n",
    "# Step 2: 划分训练集和测试集\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Step 3: 运行 Lasso 模型\n",
    "lasso = Lasso(alpha=1)  # alpha 是正则化强度的参数\n",
    "lasso.fit(X_train, y_train)\n",
    "\n",
    "# Step 4: 预测和评估模型\n",
    "y_pred = lasso.predict(X_test)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(f\"Mean Absolute Error: {mae}\")\n",
    "\n",
    "# 打印模型系数\n",
    "print(f\"Model Coefficients: {lasso.coef_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create datasets and variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Create the dataset\n",
    "data = {\n",
    "    'Month': np.arange(1, 26),\n",
    "    'Demand': [100, 112, 107, 103, 91, 85, 84, 85, 79, 81, 134, 86, 99, 89, 111, 114, 118, 163, 193, 143, 144, 202, 158, 160, 144],\n",
    "    'Advance_demand': [71, 30, 75, 64, 41, 51, 42, 51, 57, 49, 134, 52, 99, 56, 81, 79, 73, 163, 193, 99, 91, 202, 105, 101, 96]\n",
    "}\n",
    "\n",
    "# Convert the data to a pandas DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Prepare Y (target variable: Demand from month 2 to 25)\n",
    "Y = df['Demand'][1:].values\n",
    "\n",
    "# Prepare X (lagged variable: Demand from month 1 to 24)\n",
    "X = df['Demand'][:-1].values\n",
    "\n",
    "# Prepare L (advance demand from month 2 to 25 for constraints)\n",
    "L = df['Advance_demand'][1:].values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the Autoregressive Model (GLS) Predictor Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a constant term (intercept) to build the predictor matrix for the autoregressive model\n",
    "X_gls = np.vstack([np.ones_like(X), X]).T  # Convert to a 2D matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the GLS Model Objective Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the GLS model objective function (sum of squared residuals)\n",
    "def objective_gls(beta, X, Y):\n",
    "    residuals = Y - X.dot(beta)  # Calculate residuals\n",
    "    return np.sum(residuals**2)  # Return the sum of squared residuals\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Define the Constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the constraint (predicted demand >= advance demand)\n",
    "constraints = [{'type': 'ineq', 'fun': lambda beta: X_gls.dot(beta) - L}] \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solve the Optimization Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "# Initial guess for beta (same length as the number of columns in X_gls)\n",
    "beta_initial = np.zeros(X_gls.shape[1])\n",
    "\n",
    "# Solve the constrained optimization problem for GLS\n",
    "result_gls = minimize(objective_gls, beta_initial, args=(X_gls, Y), constraints=constraints)\n",
    "\n",
    "# Extract the optimal coefficients\n",
    "beta_gls_optimal = result_gls.x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict Demand and Calculate the Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal GLS coefficients: [46.57142858  1.07936508]\n",
      "GLS MAPE: 50.35%\n"
     ]
    }
   ],
   "source": [
    "# Predict demand values using the optimal coefficients and ensure predictions are not lower than advance demand\n",
    "Y_pred_gls = np.maximum(X_gls.dot(beta_gls_optimal), L)\n",
    "\n",
    "# Calculate the Mean Absolute Percentage Error (MAPE)\n",
    "mape_gls = np.mean(np.abs((Y - Y_pred_gls) / Y)) * 100\n",
    "\n",
    "# Output the results\n",
    "print(f\"Optimal GLS coefficients: {beta_gls_optimal}\")\n",
    "print(f\"GLS MAPE: {mape_gls:.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
